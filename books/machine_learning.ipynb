{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 模型评估与选择\n",
    "---\n",
    "> 在进行模型训练时，首先要确定模型的**评估方法**，主要的任务即选择以某种方式对数据集划分为训练集和测试集，常用的划分方法有留出法、交叉验证法、自助法。然后就是开始训练得出模型结果，这个结果用**性能度量**来衡量这次训练的结果，常用的性能度量有错误率和精准率。\n",
    "模型需要**评估方法** 确定训练集和测试集，需要 **性能度量** 进行错误率的选择。**用某种实验评估方法测得学习器的某个性能度量结果** ，使用 **比较检验** 选择最优度量结果。 最后需要用**交叉检验**来比较算法性能。\n",
    "\n",
    "- error rate\n",
    "- accuracy\n",
    "- error\n",
    "    - training error(empirical error)\n",
    "    - generalization error\n",
    "- fitting\n",
    "    - overfitting\n",
    "    - underfitting\n",
    "\n",
    "## 2.2 模型的评估方法\n",
    "即确定train data, validation data, test data\n",
    "\n",
    "- 留出法(hold out)\n",
    "- 交叉验证法(k-fold cross validation)\n",
    "- 自助法:从样本池中随机选择一个，作为训练集。\n",
    "- 调参\n",
    "    - 算法参数(超参数)，参数数量不多\n",
    "    - 模型参数，参数数量超多\n",
    "\n",
    "## 2.3 performance measure\n",
    "- error rate and accuracy\n",
    "- 查准率和查全率与F1(以准确率为主还是以覆盖率为主，F1则依据参数结合了两个)\n",
    "- ROC and AUC \n",
    "- 代价敏感错误率与代价曲线:用于对错误率敏感的情形。\n",
    "\n",
    "## 2.4 比较检验\n",
    "- 假设检验：用测试错误率来衡量泛化错误率\n",
    "- 交叉验证t检验\n",
    "- McNemar检验：**交叉验证t检验和McNemar检验是比较一个数据集上的两个算法的性能。**\n",
    "- Friedman检验和Nemenyi后续检验：用于**一组数据集** 上的 **多个算法性能** 的比较。\n",
    "\n",
    "## 2.5 偏差与方差\n",
    "用于解释学习算法 **泛化性能** 的一种重要工具。\n",
    "- 所谓的高偏差就是指高误差(error),对于高偏差的学习模型，误差不会随着数据集数量的增加而减少，会趋于平和，所以要考虑更换学习模型。\n",
    "- 高方差是指过拟合现象，针对高方差的问题，可以采用针对过拟合的手段解决。\n",
    "\n",
    "**应用**\n",
    "- Get more training example --> fixes high variance\n",
    "- Try small set of features --> fixes high variance\n",
    "- Try getting additional features --> fixes high bias\n",
    "- Try adding polynomial features --> fixes high bias\n",
    "- Try decreasing lambda --> fixes high bias\n",
    "- Try increasing lambda --> fixes high variance\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "# 3 线性模型\n",
    "---\n",
    "> 线性模型可以处理线性问题，分类问题(二分类，多分类)。常用的线性问题用**线性回归**模型处理，线性模型的二分类模型有**逻辑回归模型**和**线性判别模型**。在处理多分类问题时，常用做法是把多分类分解成若干个二分类，再用二分类模型来处理。在进行分类问题时，通常会遇到的问题有**类别不平衡**，即训练数据正反例数量相差过大的问题，通常有三种解决方案。\n",
    "\n",
    "## 3.2 线性回归\n",
    "\n",
    "## 3.3 对数几率回归(逻辑回归) 二分类\n",
    "线性回归常用于线性预测，而如果想用线性模型处理**分类问题**，则需要采用对数几率回归logistic regression，**二分类**。\n",
    "\n",
    "## 3.4 线性判别分析 二分类\n",
    "Linear Discriminant Analysis(LDA):线性学习方法。将样本投影到一条直线上，同类的尽可能近，异类尽可能远, **二分类**。\n",
    "\n",
    "## 3.5 多分类\n",
    "- 多把分类拆分成多个二分类\n",
    "\n",
    "## 3.6 类别不平衡\n",
    "在处理分类问题时，常因训练集中的正反例类别不平衡，影响学习模型结果，常处理的方法有：\n",
    "- 对多的样本欠采样\n",
    "- 对少的样本过采样\n",
    "- 在预测时，添加阈值\n",
    "\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "# 4 决策树\n",
    "---\n",
    "> 决策树是常见的机器学习算法，依据不同的分支划分条件的不同，生成不同的决策树算法，目前常用的决策树算法有**ID3, C4.5, CART**,分别使用了**信息增益、增益率、基尼指数**作为划分选择。在进行决策树学习算法的训练中，为处理**过拟合**问题，采用了**剪枝处理**，包括预剪枝和后剪枝。如果样本数据是连续属性，需要先进行**离散化处理**，对于样本中的缺失值，进行**缺失值处理**。最后提到了多变量决策树。\n",
    "\n",
    "## 4.2 划分选择\n",
    "确定最优划分属性。在进行分支前，我们需要确定该以那种属性作为划分依据，依据不同的划分标准，衍生出不同的决策树算法。如采用信息增益的ID3决策树算法，使用增益率的C4.5, 使用基尼指数的CART。\n",
    "\n",
    "- 信息增益：ID3\n",
    "- 增益率：C4.5\n",
    "- 基尼指数：CART\n",
    "\n",
    "## 4.3 剪枝处理\n",
    "剪枝(pruning)主要处理“过拟合”，包括预剪枝和后剪枝。\n",
    "\n",
    "- 预剪枝\n",
    "- 后剪枝\n",
    "\n",
    "## 4.4 连续与缺失值\n",
    "\n",
    "- 连续值处理：连续属性生成决策树，需要先对连续属性离散化，采用二分法。\n",
    "- 缺失值处理：处理样本中的缺失值。\n",
    "\n",
    "## 4.5 多变量决策树\n",
    "如果样本含有多个属性，则需要考虑多变量决策树。\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "# 5 神经网络\n",
    "---\n",
    "> 神经网络和多层神经网络是由多层神经元组成的网络结构，包括两个步骤：前馈神经网络，误差反向传播两个操作。在前馈传播的过程中使用到了权重和偏置，对每层的输出进行激活函数处理，在最后的输出层，常需要转化为概率分布，使用softmax。得到预测值后计算损失函数（交叉熵），使用梯度下降算法将损失更新到各层权重。常见的网络优化方式有：学习率的指数下降，防止过拟合的正则化处理，模型参数的滑动平均等。深度学习中用到的神经网络算法有CNN，DNN，RNN，LSTM等，其中CNN处理图像识别，图像处理，RNN和LSTM 处理序列数据。\n",
    "\n",
    "## 5.2 感知机和多层网络\n",
    "- 感知机：无隐藏层的神经网络\n",
    "\n",
    "## 5.3 误差逆传播算法\n",
    "error BackPropagation\n",
    "- 学习率\n",
    "- 正则化\n",
    "\n",
    "## 5.4 全局最小与局部极小\n",
    "- 多组不同参数值初始化\n",
    "- 模拟退火\n",
    "- 随机梯度下降\n",
    "\n",
    "## 5.5 常见的神经网络\n",
    "- 径向基函数网络（RBF）\n",
    "- 自适应谐振理论网络（ART）\n",
    "- 自组织映射网络（SOM）\n",
    "- 级联相关网络\n",
    "- Elman网络（或RNN）\n",
    "- Boltzmann机\n",
    "\n",
    "## 5.6 深度学习\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "# 6 支持向量机\n",
    "---\n",
    "> SVM主要用于文本分类任务（主要处理二分类任务）。支持向量机的求解通常借助于凸优化技术。寻找最优超平面。通俗意义上讲，是寻求一个超平面，使各类映射到平面的距离最大。如何用核函数实现svm？？\n",
    "\n",
    "1. 间隔与支持向量\n",
    "    - \n",
    "2. 对偶问题\n",
    "\n",
    "3. 核函数\n",
    "    - $f = exp(\\frac{\\lVert x1 - x2 \\lVert ^2}{2\\sigma ^2})$\n",
    "    - 核函数也称**相似度函数**，常说的核函数是高斯核函数。$k(x,l)$\n",
    "        - 线性核函数(作用同logistic regression)\n",
    "        - 高斯核函数\n",
    "    - 用于训练复杂的非线性函数\n",
    "4. 软间隔与正则化\n",
    "\n",
    "5. 支持向量回归\n",
    "\n",
    "6. 支持向量机使用事项(参数选择):\n",
    "    - 参数C(=$\\frac{1}{\\lambda}$)，lambda为正则化参数\n",
    "        - large C:Lower bias, high variance(过拟合)\n",
    "        - Small C:Higher bias, low variance(欠拟合)\n",
    "    - 高斯核函数中的$\\sigma{^2}$\n",
    "        - 大的sigma，核函数曲线平滑\n",
    "        - 小的sigma，曲线陡峭\n",
    "    - `from sklearn import svm`\n",
    "    - **如果训练样本数量很大，高斯核函数速度会很慢**\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "# 7 贝叶斯分类器\n",
    "---\n",
    "> 贝叶斯决策论在机器学习和模式识别中起到了重要的作用，为了便于求解，避免组合爆炸的情形，我们会假设属性间相互独立，即**朴素贝叶斯分类器**，但是现实应用中，需要考虑属性间的联系，所以在朴素贝叶斯的基础上，做了让步引入了属性间的联系，即**半朴素贝叶斯分类器**。为充分考虑属性间的关系以及属性的联合概率分布，使用了**贝叶斯网**，为不确定学习和推断提供了基本框架。在现实的应用中，样本中会存在不完整值，这时引入了**隐变量**，而**EM**算法是处理隐变量的常用方法。贝叶斯分类器与一般意义上的“贝叶斯学习”有着显著区别，前者是通过最大后验概率进行单点估计，后者则是进行分布估计\n",
    "\n",
    "## 7.1 贝叶斯决策论\n",
    "贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。\n",
    "- 判别式模型：给定条件直接建模预测。如：决策树，BP神经网络，支持向量机。\n",
    "- 生成式模型\n",
    "$$P(c \\mid x) = \\frac{P(c)P(x \\mid c)}{P(x)}$$\n",
    "其中 $P(c)$ 是先验概率，$P(x \\mid c)$ 样本x相对于类别c的类条件概率，或称为“似然”。\n",
    "\n",
    "**生成式模型考虑的是联合分布，判别式模型考虑条件分布。**\n",
    "\n",
    "## 7.2 极大似然估计\n",
    "\n",
    "## 7.3 朴素贝叶斯分类器\n",
    "假设各属性相互独立：\n",
    "$$P(c \\mid x) = \\frac{P(c)P(x \\mid c)}{P(x)} = \\frac{P(c)}{P(x)}\\prod_{i=1}^{d}P(x_i \\mid c)$$\n",
    "判定一个条件x的结果c的概率等于该结果c的条件下的所有可能条件 $c_i$ 的概率积。\n",
    "\n",
    "## 7.4 半朴素贝叶斯分类器\n",
    "如果要考虑属性间的关系，可以采用半朴素贝叶斯分类器。\n",
    "\n",
    "## 7.5 贝叶斯网\n",
    "贝叶斯网是借助于有向无环图来刻画属性之间的关系，并使用条件概率表来描述属性的联合概率分布。\n",
    "- 结构\n",
    "- 学习\n",
    "- 推断\n",
    "\n",
    "## 7.6 EM算法\n",
    "处理样本数据不完整。\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "# 8 集成学习\n",
    "---\n",
    "> 集成学习是通过构建并结合多个学习器来完成学习任务。集成学习是常见的弱学习器优化方法。依据组成学习器的不同分为基学习器和个体学习器。按照个体学习器的生成方式，可以将集成学习方法分为两类：个体学习器间存在强依赖关系，必须串行生成的**boosting**方法，常见代表有**Adaboost**主要用于二分类。不存在强依赖关系，可以并行生成的**Bagging**和**随机森林**，bagging可以适用于多分类和回归任务。随机森林主要实现的是决策树，在效率和结果上由于bagging。使用组合策略对单个学习算法进行优化，可以明显提升学习算法的效果。\n",
    "\n",
    "## 8.1 个体与集成\n",
    "- 基学习器：同质学习器的集成。\n",
    "- 个体学习器：异质学习器的集成。\n",
    "\n",
    "根据个体学习器的生成方式，目前的集成学习方法大致可以分为两类：\n",
    "- 个体学习器间存在**强依赖**关系、必须**串行**生成的序列化方法。**Boosting**\n",
    "- 个体学习器间**不存在强依赖**关系，可同时生成的**并行**化方法。**Bagging** and **Random Forest**\n",
    "\n",
    "## 8.2 Boosting\n",
    "Boosting是一族可以将弱学习器提升为强学习器的算法。最著名的算法为：**AdaBoost**只适用于二分类任务。\n",
    "\n",
    "## 8.3 Bagging与随机森林\n",
    "- Bagging是并行集成学习方法最著名代表，采用自助采样法，能适用于多分类和回归任务。在不剪枝决策树和神经网络的学习器的效果上较为明显。\n",
    "- 随机森林是bagging的一个扩展变体。RF在以**决策树**为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性。随机选择一批属性在择优。\n",
    "\n",
    "## 8.4 结合策略\n",
    "优点：\n",
    "- 从统计上看，多个学习器，避免泛化性能不佳。\n",
    "- 从计算上，多学习器可降低局部最小的风险。\n",
    "- 从表示上看，多学习器的适用性更广。\n",
    "\n",
    "方法：\n",
    "- 平均法\n",
    "- 投票法\n",
    "- 学习法\n",
    "\n",
    "## 8.5 多样性\n",
    "- 误差-分歧分解\n",
    "- 多样性度量\n",
    "- 多样性增强\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "# 9 聚类\n",
    "---\n",
    "> 聚类常用于寻找数据内在的分布结构，聚类算法主要设计两个问题：性能度量、距离计算。\n",
    "\n",
    "## 9.2 性能度量\n",
    "评估聚类结果\n",
    "\n",
    "- 性能度量外部指标\n",
    "    - Jaccard系数\n",
    "    - FM指数\n",
    "    - Rand指数\n",
    "- 性能度量内部指标\n",
    "    - DB指数\n",
    "    - Dunn指数\n",
    "    \n",
    "## 9.3 距离计算\n",
    "- 闵科夫斯基距离\n",
    "    - 欧氏距离\n",
    "    - 曼哈顿距离\n",
    "\n",
    "## 9.4 原型聚类\n",
    "- k-means算法\n",
    "- 学习向量量化(LVQ)\n",
    "- 高斯混合聚类\n",
    "\n",
    "## 9.5 密度聚类\n",
    "- DBSCAN\n",
    "\n",
    "## 9.6 层次聚类\n",
    "- AGNES\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "\n",
    "# 10 降维与度量学习\n",
    "---\n",
    "> 数据降维处理是在进行高维数据学习的主要处理方法，常用的有无监督的线性降维方法PCA，和非线性降维方法KPCA\n",
    "\n",
    "1. k近邻学习:knn是常用的监督学习方法，给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个邻居的信息进行预测。\n",
    "\n",
    "2. 低维嵌入:降维处理。\n",
    "\n",
    "3. 主成分分析:PCA,是一种无监督的线性降维方法，想要实现的是**最小化投影误差平方的平均值**\n",
    "    - 理解PCA和线性回归的区别，LR是预测值和实际值的差平方，PCA是点到线的投影，垂直投影（点到线或面的投影平方差）。\n",
    "    - PCA的步骤\n",
    "        - 去均值化处理（注意：与数据缩放的区别）\n",
    "        - 求协方差矩阵\n",
    "    - PCA的作用\n",
    "        - 压缩数据\n",
    "        - 提高训练效率\n",
    "        - *不要尝试使用PCA来处理过拟合，因为PCA是对特征值x操作，没有对y进行处理，处理过拟合最好的办法仍然是正则化。*\n",
    "        - *不要在项目的一开始就是用PCA，应该是先使用原始数据，如果效果不好，在考虑使用PCA，然后去寻找K值，确定最后应该保留的误差*\n",
    "4. 核化线性降维:KPCA是一种非线性降维，引入了核函数，进行投影降维。\n",
    "\n",
    "5. 流形学习:如果局部具有欧氏空间的关系，那么样本在高维空间也具有欧氏空间关系。那么可以先在局部建立降维映射关系，再推广到全局。\n",
    "    - 等度量映射Isomap\n",
    "    - 局部线性嵌入LLe\n",
    "\n",
    "6. 度量学习:通过度量学习确定距离度量，即合适的降维空间\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "# 11 特征选择与稀疏学习\n",
    "---\n",
    "> **特征选择**是一个重要的数据预处理过程，在机器学习任务中，首先要做的就是确定特征，然后再训练学习器。进行特征选择一方面可以降低训练维度，一方面去除不相关特征可以降低学习难度。特征选择的方法主要有过滤式选择和包裹式选择和嵌入式选择。当样本中存在很多无用的数据时，需要考虑使用**稀疏学习**，以及相应的解决方法字**典学习**。**压缩感知**主要用于数据压缩和恢复，如人脸识别的鲁棒主成分分析。\n",
    "\n",
    "## 11.1 子集搜索与评价\n",
    "- 子集搜索，根据给定的特征集合，搜索出最优特征子集，需要穷举。\n",
    "- 子集评价\n",
    "\n",
    "## 11.2 特征选择的方法\n",
    "- 过滤式选择：先过滤选择特征，再进行训练。（Relief）\n",
    "- 包裹式选择：依据训练结果，反过来选择最合适子集。(LVW)\n",
    "- 嵌入式选择：将特征选择过程，和训练过程结合到一起。L2正则化也称岭回归。\n",
    "\n",
    "## 11.3 稀疏表示与字典学习\n",
    "稀疏表示，当样本数据中有较多的零元素，需要考虑采用稀疏表示。最常见的场景是自然语言处理，常见的解决方法是创建字典。\n",
    "\n",
    "## 11.4 压缩感知\n",
    "为保留样本的的特征，会对数据进行放大处理。但是在传输的过程中，需要对数据进行压缩。主要包括两个阶段：\n",
    "- 感知测量:关注如何对原始信号进行处理以获得稀疏样本表示（傅里叶变换，小波变换，字典学习，稀疏编码）\n",
    "- 重构恢复：基于稀疏性恢复原信号。\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "# 12 计算学习理论\n",
    "---\n",
    "> 计算学习理论研究的是关于通过计算来进行学习的理论。\n",
    "\n",
    "1. PAC:计算学习理论中最基本的**概率近似正确**学习理论。\n",
    "2. 有限假设空间\n",
    "    - 可分情形\n",
    "    - 不可分情形\n",
    "3. 无限假设空间\n",
    "    - VC维：研究无限假设空间的复杂度。\n",
    "4. Rademacher复杂度：刻画假设空间复杂度的途径。\n",
    "5. 稳定性分析\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "# 13 半监督学习\n",
    "---\n",
    "> 半监督学习是指让学习器不依赖外界交互、自动利用未标记样本来提升学习性能。可分为：纯半监督学习和直推学习。\n",
    "\n",
    "1. 生成式方法：是直接基于生成式模型的方法\n",
    "2. 半监督SVM\n",
    "3. 图半监督学习\n",
    "4. 基于分歧的方法\n",
    "5. 半监督聚类\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "# 14 概率图模型\n",
    "---\n",
    "> 概率图模型是一类用图来表达变量相关关系的概率模型。常分为：有向无环图表示变量间的依赖关系，称为有向图模型或贝叶斯网；使用无向图表示变量间的相关关系，称为无向图模型或马尔可夫网。\n",
    "\n",
    "## 14.1 隐马尔科夫模型\n",
    "隐马尔科夫模型是结构最简单的贝叶斯网，主要用于时序数据建模。HMM中的变量可以分为两组:\n",
    "- 状态变量\n",
    "- 观测变量\n",
    "\n",
    "HMM需要考虑的参数：\n",
    "- 状态转移概率\n",
    "- 输出观测概率\n",
    "- 初始状态概率\n",
    "\n",
    "## 14.2 马尔可夫随机场\n",
    "MRF是典型的马尔可夫网，是著名的**无向图模型**。\n",
    "\n",
    "## 14.3 条件随机场\n",
    "CRF是一种**判别式无向图模型**，而HMM和MRF是生成式模型。\n",
    "\n",
    "## 14.4 学习与推断\n",
    "概率图模型的推断方法大致可以分为两类：\n",
    "- 精确推断方法\n",
    "    - 变量消去\n",
    "    - 信念传播\n",
    "- 近似推断方法\n",
    "    - MCMC采样\n",
    "    - 变分推断\n",
    "\n",
    "## 14.6 话题模型\n",
    "话题模型是一族生成式有向图模型，主要用于处理离散型的数据。**隐狄利克雷分配模型**\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "\n",
    "# 15 规则学习\n",
    "---\n",
    "> 与神经网络和SVM这样的黑箱模型相比，规则学习具有更好的可解释性，能使用户更直观的对判别过程有所了解。**规则学习**是符号主义学习的主要代表。\n",
    "\n",
    "1. 序贯覆盖\n",
    "1. 剪枝优化\n",
    "1. 一介规则学习\n",
    "1. 归纳逻辑程序设计\n",
    "    - 最小一般泛化\n",
    "    - 逆归结\n",
    "\n",
    "<hr style=\"border:none;border-top:3px solid #5bc0de\"/>\n",
    "\n",
    "# 16 强化学习\n",
    "---\n",
    "1. 任务与奖赏\n",
    "1. K-摇臂赌博机\n",
    "    - 探索与利用\n",
    "    - 贪心\n",
    "    - softmax\n",
    "1. 有模型学习\n",
    "    - 策略评估\n",
    "    - 策略改进\n",
    "    - 策略迭代与值迭代\n",
    "1. 免模型学习\n",
    "    - 蒙特卡洛强化学习\n",
    "    - 时序差分学习\n",
    "1. 值函数近似\n",
    "1. 模仿学习\n",
    "    - 直接模仿学习\n",
    "    - 逆强化学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 补充\n",
    "1. 高斯分布：也称正太分布\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
